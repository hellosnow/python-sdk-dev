api_name: []
items:
- _type: module
  children:
  - cntk.train.distributed.Communicator
  - cntk.train.distributed.DistributedLearner
  - cntk.train.distributed.WorkerDescriptor
  - cntk.train.distributed.block_momentum_distributed_learner
  - cntk.train.distributed.data_parallel_distributed_learner
  fullName: cntk.train.distributed
  module: cntk.train.distributed
  name: distributed
  source:
    id: distributed
    path: cntk/train/distributed.py
    remote:
      branch: master
      path: cntk/train/distributed.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 0
  type: Namespace
  uid: cntk.train.distributed
- _type: function
  fullName: cntk.train.distributed.block_momentum_distributed_learner
  module: cntk.train.distributed
  name: block_momentum_distributed_learner
  source:
    id: block_momentum_distributed_learner
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 141
  syntax:
    exceptions: []
    parameters:
    - description: a local learner (i.e. sgd)
      id: use_nestrov_momentum
      type: bool
    - description: size of the partition in samples
      id: block_size
      type: int
    - description: block momentum as time constant
      id: reset_sgd_momentum_after_aggregation
      type: bool
    - description: use nestrov momentum
      id: block_learning_rate
      type: float
    - description: reset SGD momentum after aggregation
      id: block_momentum_as_time_constant
      type: float
    - description: block learning rate
      id: distributed_after
      type: int
    returntype: ''
    returnvalue: a distributed learner instance
    summary: 'Creates a block momentum distributed learner. See [1] for more information.

      Block Momentum divides the full dataset into M non-overlapping blocks, and each
      block is partitioned into N non-overlapping splits.

      During training, a random, unprocessed block is randomly taken by the trainer
      and the N partitions of this block are dispatched on the workers.

      See also: [1] K. Chen and Q. Huo. [Scalable training of deep learning machines
      by incremental block training with intra-block parallel optimization and blockwise
      model-update filtering](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/0005880.pdf).
      Proceedings of ICASSP, 2016.'
    variables: []
  type: Method
  uid: cntk.train.distributed.block_momentum_distributed_learner
- _type: function
  fullName: cntk.train.distributed.data_parallel_distributed_learner
  module: cntk.train.distributed
  name: data_parallel_distributed_learner
  source:
    id: data_parallel_distributed_learner
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 115
  syntax:
    exceptions: []
    parameters:
    - description: a local learner (i.e. sgd)
      id: num_quantization_bits
      type: int
    - description: number of samples after which distributed training starts
      id: distributed_after
      type: int
    - description: number of bits for quantization (1 to 32)
      id: use_async_buffered_parameter_update
      type: bool
    returntype: ''
    returnvalue: a distributed learner instance
    summary: Creates a data parallel distributed learner
    variables: []
  type: Method
  uid: cntk.train.distributed.data_parallel_distributed_learner
references:
- fullName: cntk.train.distributed.Communicator
  isExternal: false
  name: Communicator
  parent: cntk.train.distributed
  uid: cntk.train.distributed.Communicator
- fullName: cntk.train.distributed.DistributedLearner
  isExternal: false
  name: DistributedLearner
  parent: cntk.train.distributed
  uid: cntk.train.distributed.DistributedLearner
- fullName: cntk.train.distributed.WorkerDescriptor
  isExternal: false
  name: WorkerDescriptor
  parent: cntk.train.distributed
  uid: cntk.train.distributed.WorkerDescriptor
- fullName: cntk.train.distributed.block_momentum_distributed_learner
  isExternal: false
  name: block_momentum_distributed_learner
  parent: cntk.train.distributed
  uid: cntk.train.distributed.block_momentum_distributed_learner
- fullName: cntk.train.distributed.data_parallel_distributed_learner
  isExternal: false
  name: data_parallel_distributed_learner
  parent: cntk.train.distributed
  uid: cntk.train.distributed.data_parallel_distributed_learner
