api_name: []
items:
- _type: module
  children:
  - cntk.learners.Learner
  - cntk.learners.UnitType
  - cntk.learners.UserLearner
  - cntk.learners.adagrad
  - cntk.learners.adam
  - cntk.learners.adam_sgd
  - cntk.learners.default_unit_gain_value
  - cntk.learners.fsadagrad
  - cntk.learners.learning_rate_schedule
  - cntk.learners.momentum_as_time_constant_schedule
  - cntk.learners.momentum_schedule
  - cntk.learners.momentum_sgd
  - cntk.learners.nesterov
  - cntk.learners.rmsprop
  - cntk.learners.set_default_unit_gain_value
  - cntk.learners.sgd
  - cntk.learners.training_parameter_schedule
  fullName: cntk.learners
  module: cntk.learners
  name: learners
  references:
  - fullName: cntk.learners.Learner
    isExternal: false
    name: Learner
    parent: ''
    uid: cntk.learners.Learner
  - fullName: cntk.learners.UnitType
    isExternal: false
    name: UnitType
    parent: ''
    uid: cntk.learners.UnitType
  - fullName: cntk.learners.UserLearner
    isExternal: false
    name: UserLearner
    parent: ''
    uid: cntk.learners.UserLearner
  - fullName: cntk.learners.adagrad
    isExternal: false
    name: adagrad
    parent: ''
    uid: cntk.learners.adagrad
  - fullName: cntk.learners.adam
    isExternal: false
    name: adam
    parent: ''
    uid: cntk.learners.adam
  - fullName: cntk.learners.adam_sgd
    isExternal: false
    name: adam_sgd
    parent: ''
    uid: cntk.learners.adam_sgd
  - fullName: cntk.learners.default_unit_gain_value
    isExternal: false
    name: default_unit_gain_value
    parent: ''
    uid: cntk.learners.default_unit_gain_value
  - fullName: cntk.learners.fsadagrad
    isExternal: false
    name: fsadagrad
    parent: ''
    uid: cntk.learners.fsadagrad
  - fullName: cntk.learners.learning_rate_schedule
    isExternal: false
    name: learning_rate_schedule
    parent: ''
    uid: cntk.learners.learning_rate_schedule
  - fullName: cntk.learners.momentum_as_time_constant_schedule
    isExternal: false
    name: momentum_as_time_constant_schedule
    parent: ''
    uid: cntk.learners.momentum_as_time_constant_schedule
  - fullName: cntk.learners.momentum_schedule
    isExternal: false
    name: momentum_schedule
    parent: ''
    uid: cntk.learners.momentum_schedule
  - fullName: cntk.learners.momentum_sgd
    isExternal: false
    name: momentum_sgd
    parent: ''
    uid: cntk.learners.momentum_sgd
  - fullName: cntk.learners.nesterov
    isExternal: false
    name: nesterov
    parent: ''
    uid: cntk.learners.nesterov
  - fullName: cntk.learners.rmsprop
    isExternal: false
    name: rmsprop
    parent: ''
    uid: cntk.learners.rmsprop
  - fullName: cntk.learners.set_default_unit_gain_value
    isExternal: false
    name: set_default_unit_gain_value
    parent: ''
    uid: cntk.learners.set_default_unit_gain_value
  - fullName: cntk.learners.sgd
    isExternal: false
    name: sgd
    parent: ''
    uid: cntk.learners.sgd
  - fullName: cntk.learners.training_parameter_schedule
    isExternal: false
    name: training_parameter_schedule
    parent: ''
    uid: cntk.learners.training_parameter_schedule
  source:
    id: learners
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/learners/__init__.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/learners/__init__.py
      repo: null
    startLine: 0
  type: Namespace
  uid: cntk.learners
- _type: function
  fullName: cntk.learners.adagrad
  module: cntk.learners
  name: adagrad
  source:
    id: adagrad
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 562
  syntax:
    exceptions: []
    parameters:
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: lr
      type: output of learning_rate_schedule()
    - description: learning rate schedule.
      id: l2_regularization_weight
      type: float, optional
    - description: ''
      id: need_ave_multiplier
      type: bool, default
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: gradient_clipping_with_truncation
      type: bool, default True
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type: float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: parameters
      type: list of parameters
    - description: clipping threshold per sample, defaults to infinity
      id: gradient_clipping_threshold_per_sample
      type: float, optional
    - description: use gradient clipping with truncation
      id: gaussian_noise_injection_std_dev
      type: float, optional
    returntype: ''
    returnvalue: Instance of a `Learner` that can be passed to the `Trainer`
    summary: 'Creates an AdaGrad learner instance to learn the parameters. See [1]
      for more information.

      See also: [1]  J. Duchi, E. Hazan, and Y. Singer. [Adaptive Subgradient Methods
      for Online Learning and Stochastic Optimization](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf).
      The Journal of Machine Learning Research, 2011.'
    variables: []
  type: Method
  uid: cntk.learners.adagrad
- _type: function
  fullName: cntk.learners.adam
  module: cntk.learners
  name: adam
  source:
    id: adam
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 665
  syntax:
    exceptions: []
    parameters:
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: lr
      type: output of learning_rate_schedule()
    - description: learning rate schedule.
      id: variance_momentum
      type: output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: momentum schedule. For additional information, please refer to
        the [wiki](https://github.com/Microsoft/CNTK/wiki/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).
      id: momentum
      type: output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: when `True`, momentum is interpreted as a unit-gain filter. Defaults
        to the value returned by `default_unit_gain_value()`.
      id: l1_regularization_weight
      type: float, optional
    - description: variance momentum schedule. Defaults to `momentum_as_time_constant_schedule(720000)`.
      id: parameters
      type: list of parameters
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type: float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: gradient_clipping_with_truncation
      type: bool, default True
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gradient_clipping_threshold_per_sample
      type: float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gaussian_noise_injection_std_dev
      type: float, optional
    returntype: ''
    returnvalue: Instance of a `Learner` that can be passed to the `Trainer`
    summary: 'Creates an Adam learner instance to learn the parameters. See [1] for
      more information.

      See also: [1] D. Kingma, J. Ba. [Adam: A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980).
      International Conference for Learning Representations, 2015.'
    variables: []
  type: Method
  uid: cntk.learners.adam
- _type: function
  fullName: cntk.learners.adam_sgd
  module: cntk.learners
  name: adam_sgd
  source:
    id: adam_sgd
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 723
  syntax:
    exceptions: []
    parameters:
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: lr
      type: output of learning_rate_schedule()
    - description: learning rate schedule.
      id: variance_momentum
      type: output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: '<SYSTEM MESSAGE: /root/cntk-python/doc/cntk.learners.rst:2: (INFO/1)
        Duplicate explicit target name: "wiki".>


        momentum schedule. For additional information, please refer to the [wiki](https://github.com/Microsoft/CNTK/wiki/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).'
      id: momentum
      type: output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: when `True`, momentum is interpreted as a unit-gain filter. Defaults
        to the value returned by `default_unit_gain_value()`.
      id: l1_regularization_weight
      type: float, optional
    - description: variance momentum schedule. Defaults to `momentum_as_time_constant_schedule(720000)`.
      id: parameters
      type: list of parameters
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type: float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: gradient_clipping_with_truncation
      type: bool, default True
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gradient_clipping_threshold_per_sample
      type: float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gaussian_noise_injection_std_dev
      type: float, optional
    returntype: ''
    returnvalue: Instance of a `Learner` that can be passed to the `Trainer`
    summary: "DEPRECATED.\nadam_sgd(parameters, lr, momentum, unit_gain=default_unit_gain_value(),\
      \ variance_momentum=momentum_as_time_constant_schedule(720000), low_memory=True,\
      \ l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0,\
      \ gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)\
      \ Creates an Adam learner if low_memory is False or FSAdaGrad otherwise to learn\
      \ the parameters. See [1] for more information.\nSee also: <SYSTEM MESSAGE:\
      \ /root/cntk-python/doc/cntk.learners.rst:2: (INFO/1) Duplicate explicit target\
      \ name: \"adam: a method for stochastic optimization\".>\n\n  [1] D. Kingma,\
      \ J. Ba. [Adam: A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980).\
      \ International Conference for Learning Representations, 2015."
    variables: []
  type: Method
  uid: cntk.learners.adam_sgd
- _type: function
  fullName: cntk.learners.default_unit_gain_value
  module: cntk.learners
  name: default_unit_gain_value
  source:
    id: default_unit_gain_value
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/learners/__init__.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/learners/__init__.py
      repo: null
    startLine: 59
  syntax:
    summary: Returns true if by default momentum is applied in the unit-gain fashion.
  type: Method
  uid: cntk.learners.default_unit_gain_value
- _type: function
  fullName: cntk.learners.fsadagrad
  module: cntk.learners
  name: fsadagrad
  source:
    id: fsadagrad
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 612
  syntax:
    exceptions: []
    parameters:
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: lr
      type: output of learning_rate_schedule()
    - description: learning rate schedule.
      id: variance_momentum
      type: output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: '<SYSTEM MESSAGE: /root/cntk-python/doc/cntk.learners.rst:2: (INFO/1)
        Duplicate explicit target name: "wiki".>


        momentum schedule. For additional information, please refer to the [wiki](https://github.com/Microsoft/CNTK/wiki/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).'
      id: momentum
      type: output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: when `True`, momentum is interpreted as a unit-gain filter. Defaults
        to the value returned by `default_unit_gain_value()`.
      id: l1_regularization_weight
      type: float, optional
    - description: variance momentum schedule. Defaults to `momentum_as_time_constant_schedule(720000)`.
      id: parameters
      type: list of parameters
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type: float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: gradient_clipping_with_truncation
      type: bool, default True
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gradient_clipping_threshold_per_sample
      type: float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gaussian_noise_injection_std_dev
      type: float, optional
    returntype: ''
    returnvalue: Instance of a `Learner` that can be passed to the `Trainer`
    summary: Creates an FSAdaGrad learner instance to learn the parameters.
    variables: []
  type: Method
  uid: cntk.learners.fsadagrad
- _type: function
  fullName: cntk.learners.learning_rate_schedule
  module: cntk.learners
  name: learning_rate_schedule
  source:
    id: learning_rate_schedule
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 293
  syntax:
    exceptions: []
    parameters:
    - description: see parameter `schedule` in `training_parameter_schedule()`.
      id: lr
      type: float or list
    - description: see parameter `unit` in `training_parameter_schedule()`.
      id: unit
      type: UnitType
    - description: see parameter `epoch_size` in `training_parameter_schedule()`.
      id: epoch_size
      type: int
    returntype: ''
    returnvalue: learning rate schedule
    summary: 'Create a learning rate schedule (using the same semantics as `training_parameter_schedule()`).

      See also: `training_parameter_schedule()`'
    variables: []
  type: Method
  uid: cntk.learners.learning_rate_schedule
- _type: function
  fullName: cntk.learners.momentum_as_time_constant_schedule
  module: cntk.learners
  name: momentum_as_time_constant_schedule
  source:
    id: momentum_as_time_constant_schedule
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 354
  syntax:
    exceptions: []
    parameters: []
    returntype: ''
    returnvalue: momentum as time constant schedule
    summary: 'Create a momentum schedule in a minibatch-size agnostic way (using the
      same semantics as `training_parameter_schedule()` with *unit=UnitType.sample*).

      CNTK specifies momentum in a minibatch-size agnostic way as the time constant
      (in samples) of a unit-gain 1st-order IIR filter. The value specifies the number
      of samples after which a gradient has an effect of 1/e=37%.

      If you want to specify the momentum per sample (or per minibatch), use `momentum_schedule()`.

      -[ Examples ]-

      >>> # Use a fixed momentum of 1100 for all samples

      >>> m = momentum_as_time_constant_schedule(1100)

      >>> # Use the time constant 1100 for the first 1000 samples,

      >>> # then 1500 for the remaining ones

      >>> m = momentum_as_time_constant_schedule([1100, 1500], 1000)'
    variables: []
  type: Method
  uid: cntk.learners.momentum_as_time_constant_schedule
- _type: function
  fullName: cntk.learners.momentum_schedule
  module: cntk.learners
  name: momentum_schedule
  source:
    id: momentum_schedule
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 316
  syntax:
    exceptions: []
    parameters: []
    returntype: ''
    returnvalue: momentum schedule
    summary: 'Create a per-minibatch momentum schedule (using the same semantics as
      `training_parameter_schedule()` with the *unit=UnitType.minibatch*).

      If you want to provide momentum values in a minibatch-size agnostic way, use
      `momentum_as_time_constant_schedule()`.

      -[ Examples ]-

      >>> # Use a fixed momentum of 0.99 for all samples

      >>> m = momentum_schedule(0.99)

      >>> # Use the momentum value 0.99 for the first 1000 samples,

      >>> # then 0.9 for the remaining ones

      >>> m = momentum_schedule([0.99,0.9], 1000)

      >>> m[0], m[999], m[1000], m[1001]

      (0.99, 0.99, 0.9, 0.9)

      >>> # Use the momentum value 0.99 for the first 999 samples,

      >>> # then 0.88 for the next 888 samples, and 0.77 for the

      >>> # the remaining ones

      >>> m = momentum_schedule([(999,0.99),(888,0.88),(0, 0.77)])

      >>> m[0], m[998], m[999], m[999+888-1], m[999+888]

      (0.99, 0.99, 0.88, 0.88, 0.77)'
    variables: []
  type: Method
  uid: cntk.learners.momentum_schedule
- _type: function
  fullName: cntk.learners.momentum_sgd
  module: cntk.learners
  name: momentum_sgd
  source:
    id: momentum_sgd
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 453
  syntax:
    exceptions: []
    parameters:
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: lr
      type: output of learning_rate_schedule()
    - description: learning rate schedule.
      id: l2_regularization_weight
      type: float, optional
    - description: '<SYSTEM MESSAGE: /root/cntk-python/doc/cntk.learners.rst:2: (INFO/1)
        Duplicate explicit target name: "wiki".>


        momentum schedule. For additional information, please refer to the [wiki](https://github.com/Microsoft/CNTK/wiki/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).'
      id: momentum
      type: output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: when `True`, momentum is interpreted as a unit-gain filter. Defaults
        to the value returned by `default_unit_gain_value()`.
      id: l1_regularization_weight
      type: float, optional
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: parameters
      type: list of parameters
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: gradient_clipping_threshold_per_sample
      type: float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gradient_clipping_with_truncation
      type: bool, default True
    - description: clipping threshold per sample, defaults to infinity
      id: gaussian_noise_injection_std_dev
      type: float, optional
    returntype: ''
    returnvalue: Instance of a `Learner` that can be passed to the `Trainer`
    summary: Creates a Momentum SGD learner instance to learn the parameters.
    variables: []
  type: Method
  uid: cntk.learners.momentum_sgd
- _type: function
  fullName: cntk.learners.nesterov
  module: cntk.learners
  name: nesterov
  source:
    id: nesterov
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 502
  syntax:
    exceptions: []
    parameters:
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: lr
      type: output of learning_rate_schedule()
    - description: learning rate schedule.
      id: l2_regularization_weight
      type: float, optional
    - description: '<SYSTEM MESSAGE: /root/cntk-python/doc/cntk.learners.rst:2: (INFO/1)
        Duplicate explicit target name: "wiki".>


        momentum schedule. For additional information, please refer to the [wiki](https://github.com/Microsoft/CNTK/wiki/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).'
      id: momentum
      type: output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: when `True`, momentum is interpreted as a unit-gain filter. Defaults
        to the value returned by `default_unit_gain_value()`.
      id: l1_regularization_weight
      type: float, optional
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: parameters
      type: list of parameters
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: gradient_clipping_threshold_per_sample
      type: float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gradient_clipping_with_truncation
      type: bool, default True
    - description: clipping threshold per sample, defaults to infinity
      id: gaussian_noise_injection_std_dev
      type: float, optional
    returntype: ''
    returnvalue: Instance of a `Learner` that can be passed to the `Trainer`.
    summary: "Creates a Nesterov SGD learner instance to learn the parameters. This\
      \ was originally proposed by Nesterov [1] in 1983 and then shown to work well\
      \ in a deep learning context by Sutskever, et al. [2].\nSee also: [1] Y. Nesterov.\
      \ A Method of Solving a Convex Programming Problem with Convergence Rate O(1/\
      \ sqrt(k)). Soviet Mathematics Doklady, 1983.\n\n  [2] I. Sutskever, J. Martens,\
      \ G. Dahl, and G. Hinton. [On the Importance of Initialization and Momentum\
      \ in Deep Learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf).  Proceedings\
      \ of the 30th International Conference on Machine Learning, 2013."
    variables: []
  type: Method
  uid: cntk.learners.nesterov
- _type: function
  fullName: cntk.learners.rmsprop
  module: cntk.learners
  name: rmsprop
  source:
    id: rmsprop
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 785
  syntax:
    exceptions: []
    parameters:
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: l2_regularization_weight
      type: float, optional
    - description: learning rate schedule.
      id: min
      type: float
    - description: Trade-off factor for current and previous gradients. Common value
        is 0.95
      id: need_ave_multiplier
      type: bool, default True
    - description: Increasing factor when trying to adjust current learning_rate
      id: gradient_clipping_with_truncation
      type: bool, default True
    - description: Decreasing factor when trying to adjust current learning_rate
      id: dec
      type: float
    - description: Maximum scale allowed for the initial learning_rate
      id: parameters
      type: list of parameters
    - description: Minimum scale allowed for the initial learning_rate
      id: gaussian_noise_injection_std_dev
      type: float, optional
    - description: ''
      id: lr
      type: output of learning_rate_schedule()
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: inc
      type: float
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type: float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gamma
      type: float
    - description: clipping threshold per sample, defaults to infinity
      id: max
      type: float
    - description: use gradient clipping with truncation
      id: gradient_clipping_threshold_per_sample
      type: float, optional
    returntype: ''
    returnvalue: Instance of a `Learner` that can be passed to the `Trainer`
    summary: Creates an RMSProp learner instance to learn the parameters.
    variables: []
  type: Method
  uid: cntk.learners.rmsprop
- _type: function
  fullName: cntk.learners.set_default_unit_gain_value
  module: cntk.learners
  name: set_default_unit_gain_value
  source:
    id: set_default_unit_gain_value
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/learners/__init__.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/learners/__init__.py
      repo: null
    startLine: 66
  syntax:
    summary: Sets globally default unit-gain flag value.
  type: Method
  uid: cntk.learners.set_default_unit_gain_value
- _type: function
  fullName: cntk.learners.sgd
  module: cntk.learners
  name: sgd
  source:
    id: sgd
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 405
  syntax:
    exceptions: []
    parameters:
    - description: list of network parameters to tune. These can be obtained by the
        '.parameters()' method of the root operator.
      id: lr
      type: output of learning_rate_schedule()
    - description: learning rate schedule.
      id: l2_regularization_weight
      type: float, optional
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: gradient_clipping_with_truncation
      type: bool, default True
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type: float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: parameters
      type: list of parameters
    - description: clipping threshold per sample, defaults to infinity
      id: gradient_clipping_threshold_per_sample
      type: float, optional
    - description: use gradient clipping with truncation
      id: gaussian_noise_injection_std_dev
      type: float, optional
    returntype: ''
    returnvalue: Instance of a `Learner` that can be passed to the `Trainer`
    summary: 'Creates an SGD learner instance to learn the parameters. See [1] for
      more information on how to set the parameters.

      See also: [1] L. Bottou. [Stochastic Gradient Descent Tricks](http://research.microsoft.com/pubs/192769/tricks-2012.pdf).
      Neural Networks: Tricks of the Trade: Springer, 2012.'
    variables: []
  type: Method
  uid: cntk.learners.sgd
- _type: function
  fullName: cntk.learners.training_parameter_schedule
  module: cntk.learners
  name: training_parameter_schedule
  source:
    id: training_parameter_schedule
    path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
    remote:
      branch: '/bin/sh: 1: git: not found'
      path: root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py
      repo: null
    startLine: 220
  syntax:
    exceptions: []
    parameters:
    - description: if float, is the parameter schedule to be used for all samples.
        In case of list, the elements are used as the values for `epoch_size` samples.
        If list contains pair, the second element is used as a value for (`epoch_size`
        x first element) samples
      id: epoch_size
      type: optional, int
    - description: 'one of two * `sample`: the returned schedule contains per-sample
        values * `minibatch`: the returned schedule contains per-minibatch values.'
      id: unit
      type: UnitType
    - description: number of samples as a scheduling unit. Parameters in the schedule
        change their values every `epoch_size` samples. If no `epoch_size` is provided,
        this parameter is substituted by the size of the full data sweep, in which
        case the scheduling unit is the entire data sweep (as indicated by the MinibatchSource)
        and parameters change their values on the sweep-by-sweep basis specified by
        the `schedule`.
      id: schedule
      type: float or list
    returntype: ''
    returnvalue: training parameter schedule
    summary: 'Create a training parameter schedule containing either per-sample (default)
      or per-minibatch values.

      -[ Examples ]-

      >>> # Use a fixed value 0.01 for all samples

      >>> s = training_parameter_schedule(0.01, UnitType.sample)

      >>> s[0], s[1]

      (0.01, 0.01)

      >>> # Use 0.01 for the first 1000 samples, then 0.001 for the remaining ones

      >>> s = training_parameter_schedule([0.01, 0.001], UnitType.sample, 1000)

      >>> s[0], s[1], s[1000], s[1001]

      (0.01, 0.01, 0.001, 0.001)

      >>> # Use 0.1 for the first 12 epochs, then 0.01 for the next 15,

      >>> # followed by 0.001 for the remaining ones, with a 100 samples in an epoch

      >>> s = training_parameter_schedule([(12, 0.1), (15, 0.01), (1, 0.001)], UnitType.sample,
      100)

      >>> s[0], s[1199], s[1200], s[2699], s[2700], s[5000]

      (0.1, 0.1, 0.01, 0.01, 0.001, 0.001)

      See also: `learning_rate_schedule()`'
    variables: []
  type: Method
  uid: cntk.learners.training_parameter_schedule
